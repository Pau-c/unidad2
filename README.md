# PresentaciÃ³n
<img width="1024" height="520" alt="Copia de Cheers!" src="https://github.com/user-attachments/assets/bc04834d-28b5-401d-a99a-4737208591eb" />



---

## Integrantes

| Nombre                        | Correo ElectrÃ³nico             | Usuario Github |
|-------------------------------|--------------------------------|----------------|
| Ariel Omar Leche              | ariel.leche@gmail.com          | ARielleche     |
| Diego Ariel Gutierrez         | dgutierrez.m79@gmail.com       | Diego-wert89   |
| JosÃ© Alberto Rubio            | rubiojosealberto@gmail.com     | rubiojar       |
| Maria Paula Coba              | mpcobas@gmail.com              | Pau-c          |
| Mauro Ruben De Natale         | maurodenatale@gmail.com        | M3T30R0-dev    |



> [!TIP]
> [Live Demo](https://deepnote.com/workspace/test-a96860ea-b228-4e9e-a13d-9edd20f60d93/project/MP-Cs-Untitled-project-a2cd30de-aa34-492f-bd7f-baf47d78ec21/notebook/ej3u2-47c02c388b264ca69ce0f37ecfdb9215?utm_content=a2cd30de-aa34-492f-bd7f-baf47d78ec21)

## DescripciÃ³n

En este proyecto de python se analizan las tendencias musicales en streaming medida por Billboard desde el aÃ±o 2013 hasta 2025.
Se  guarda el dataset en Supabase, se lo recupera en un dataframe y se lo trabaja en un notebook de Jupyter para su exploraciÃ³n y visualizaciÃ³n.

## Instrucciones 
### - Bajar dataset, nos vamos a centrar en el archivo `streaming_songs.csv`
>[Dataset original](https://www.kaggle.com/datasets/ludmin/billboard?resource=download)

## PROCESO ETL 

      ##Proceso exploratorio

            Nombre de archivo CVS descargado= streaming_songs.csv
            Cantidad de columnas: 8
            Date,Song,Artist,Rank,Last Week,Peak Position,Weeks in Charts,Image URL
            Cantidad de registros:33050
            Diccionario de dataset (funciÃ³n de cada columna y tipo de dato) 
            Limpieza ( se borra Ãºltima columna  â€œ image URLâ€ )   
            Cantidad de registros (null o faltantes) por columna  
                    Date(100% ok),Song,Artist(100% ok),Rank(100% ok),Last Week(100% ok),Peak Position(100% ok),Weeks in Charts(100% ok)

``` 
      
      # DICCIONARIO DE DATOS DEL DATASET
            
                    1-Date: Tipo: Fecha (formato yyyy/mm/dd) DescripciÃ³n: Fecha en la que se registrÃ³ el ranking de la canciÃ³n.
                    2-Song: Tipo: Texto (string) DescripciÃ³n: Nombre de la canciÃ³n en el ranking.
                    3-Artist: Tipo: Texto (string) DescripciÃ³n: Nombre del artista o grupo musical (puede incluir â€œFeaturingâ€ sÃ­ hay colaboraciones).
                    4-Rank: Tipo: NumÃ©rico entero DescripciÃ³n: PosiciÃ³n actual de la canciÃ³n en el ranking (1 = primer lugar).
                    5-Last_Week:Tipo: NumÃ©rico entero  DescripciÃ³n: PosiciÃ³n que ocupaba la canciÃ³n en la semana anterior.
                    6-Peak_Position:Tipo: NumÃ©rico entero  DescripciÃ³n: Mejor posiciÃ³n alcanzada por la canciÃ³n en el ranking hasta la fecha.
                    7-Weeks_in_Charts:Tipo: NumÃ©rico entero DescripciÃ³n: NÃºmero de semanas que la canciÃ³n lleva en el ranking.

              
```	
      ##   Proceso de TransformaciÃ³n 
		    Nota: usar cualquier herramienta de ediciÃ³n o cÃ³digo.
		    Limpieza ( se borra Ãºltima columna  â€œ image URLâ€)
		    Editar CSV  creando  columna ID (tabulaciÃ³n â€œId,â€)
			                        ID,Date,Song,Artist,Rank,Last Week,Peak_Position,Weeks_in_Charts
		    Editar campos null o con â€œ-â€ (guiÃ²n) con nÃºmero 0
		    Nombre de columnas colocar â€œ_â€ en sus nombres de las columnas
                                    ID,Date,Song,Artist,Rank,Last Week,Peak_Position,Weeks_in_Charts
            Generar nuevo archivo .CSV con nombre= dataset_artistas_CSV_PARA_BD.csv

     
---
## Base de datos en Supabase 
- crear nuevo proyecto: elegir region y password (generar password, copiarlo y guardarlo)
- Ir a la barra de la izq, table editor, crear nueva tabla, elegir nombre: `Dataset_Ranking`, click en boton importar data de .CSV -> elegir archivo `dataset_artistas_CSV` y guardar.
- En barra lateral ir a `project settings` -> api keys y copiar la de anon public
- En barra lateral ir a `data api`: copiar direccion de project url para usar en los siguientes pasos.
- Para que se tenga acceso a la tabla en la barra lateral ir a authentication -> policies y `Enable read access for all users`
  Nota: se encuentra scrip de creacion de tabla e insert de los 33050 registros en repositorio=> unidad2\archivos\Creacion_e_insert_Dataset_Ranking_full.sql , podria usar la consola de supabase de 1000 regristros para insert manualmente.
---
``` 
## âš™ï¸ Estructura del Repositorio

UNIDAD2/
â”œâ”€â”€ .github/
â”œâ”€â”€ .ipynb_checkpoints/
â”‚   â””â”€â”€ ej3u2-checkpoint.ipynb
â”œâ”€â”€ .venv/
â”‚   â”œâ”€â”€ etc/
â”‚   â”œâ”€â”€ Lib/
â”‚   â”œâ”€â”€ Scripts/
â”‚   â””â”€â”€ share/
â”œâ”€â”€ .lock
â”œâ”€â”€ pyvenv.cfg
â”œâ”€â”€ archivos/
â”‚   â”œâ”€â”€ Creacion_e_insert_Dataset_Ranking_full.sql
â”‚   â”œâ”€â”€ dataset_artistas_CSV.csv
â”‚   â””â”€â”€ streaming_songs_original.csv
â”œâ”€â”€ unidad2/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ ej3u2.ipynb   =>>>   archivo principal NOTEBOOK
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ uv.lock
``` 
## ğŸ› ï¸ Requisitos e InstalaciÃ³n

<!-- PROJECT SHIELDS -->
[![deepnoteBadge][deepnote-shield]][deepnote-url]
[![pythonBadge][python-shield]][python-url]
[![pandasBadge][pandas-shield]][pandas-url]
[![plotlyBadge][plotly-shield]][plotly-url]
[![uvBadge][uv-shield]][uv-url]
[![supabaseBadge][supabase-shield]][supabase-url]
[![jupyterBadge][jupyter-shield]][jupyter-url]
[![dotenvBadge][dotenv-shield]][dotenv-url]
<!-- PROJECT SHIELDS -->

***************************************************************************************************************
PASOS PARA USAR IDE con UV y COMENZAR DESA
****************************************************************************************************************

### - Instalar uv en la pc si es necesario (en terminal de Windows):

   Abrir un pront CMD en win  o temrinal ID, ver el pront con PS:>    y ejecutar
```
 powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### - Clonar repo:
```
git clone https://github.com/Pau-c/unidad2.git
```


### - Abrir proyecto en un IDE
### - Crear un archivo en la raÃ­z del proyecto llamado .env
- Dentro pegar la url y la key de supabase que se copiaron antes siguiendo este formato (sin espacios ni comillas):
>SUPABASE_URL=https://nbctuthbio.supabase.co

>SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6I

### - Crear entorno virtual:
Si todavÃ­a no se instalÃ³ uv en el sistema como en el primer paso, en terminal: `pip install uv` y luego  ejecutar:
```
uv venv
```

### - Sincronizar entorno virtual al instalar el proyecto y luego de cada cambio en el .toml
```
uv sync
```

### -Elegir kernel
>Para que el cÃ³digo del notebook funcione,  indicarle a VS Code que utilice el Python >que estÃ¡ dentro del nuevo entorno. Esto se hace dentro de la interfaz del notebook:

> [!IMPORTANT]
> Abrir el archivo `.ipynb` en VS Code.
> 1. Hacer clic en el selector del kernel (la esquina superior derecha).
> 2. Seleccionar otro kernel.
> 3. Elegir el kernel que tenga el nombre del proyecto.


#### asegurarse estar en el etorno correcto y activado: De ser necesario correr en terminal: `.venv\Scripts\activate`

## correr archivo en terminal con:

```
 uv run --env-file .env jupyter lab
```

> - Una vez abierto el notebook .ipynb en el browser, en la esquina superior derecha Hacer clic en `ipkernel` y elegir `start python kernel -Python 3`. de ser necesario ir al menÃº: run -> `restart kernel and run all cells`


<!-- PROJECT SHIELDS VARIABLES-->
[deepnote-shield]:https://img.shields.io/badge/Live-Deepnote-black?style=flat&labelColor=%23808080k&color=de6d40&logo=deepnote&logoColor=white
[deepnote-url]: https://deepnote.com/
[dotenv-shield]:https://img.shields.io/badge/env-dotenv-black?style=flat&labelColor=%23808080k&color=fec260&logo=dotenv&logoColor=white
[dotenv-url]:https://pypi.org/project/python-dotenv/
[pandas-shield]:https://img.shields.io/badge/Data_analysis-Pandas-black?style=flat&labelColor=%23808080k&color=453076&logo=pandas
[pandas-url]:https://pandas.pydata.org/
[python-shield]:https://img.shields.io/badge/Language-Python-black?style=flat&labelColor=%23808080k&color=2a0944&logo=python&logoColor=white
[python-url]: https://www.python.org/
[plotly-shield]:https://img.shields.io/badge/Data_Viz-Plotly-black?style=flat&labelColor=%23808080k&color=9ABF80&logo=plotly&logoColor=white
[plotly-url]: https://plotly.com/python/
[uv-shield]:https://img.shields.io/badge/Dependencias-UV-black?style=flat&labelColor=%23808080k&color=2a0944&logo=uv&logoColor=white
[uv-url]: https://github.com/astral-sh/uv
[supabase-shield]:https://img.shields.io/badge/DB-supabase-black?style=flat&labelColor=%23808080k&color=166866&logo=supabase&logoColor=white
[supabase-url]: https://supabase.com/
[jupyter-shield]:https://img.shields.io/badge/Notebook-jupyter-black?style=flat&labelColor=%23808080k&color=fec260&logo=Jupyter&logoColor=white
[jupyter-url]: https://jupyter.org/


## ğŸ”„ Flujo de Trabajo del Proyecto

Este diagrama ilustra el proceso completo, desde la obtenciÃ³n de datos hasta la visualizaciÃ³n final, pasando por las etapas clave de ETL y desarrollo en el entorno `uv`.

```mermaid
graph TD
    %% Estilos
    classDef source fill:#F9E79F,stroke:#D68910,stroke-width:2px;
    classDef etl fill:#A9CCE3,stroke:#3498DB,stroke-width:2px;
    classDef dev fill:#D5F5E3,stroke:#2ECC71,stroke-width:2px;
    classDef prod fill:#FADBD8,stroke:#E74C3C,stroke-width:2px;

    %% Nodos
    A[Dataset Original - Kaggle] --> B[Descarga streaming_songs.csv];
    B -- Archivo CSV --> C{PROCESO ETL - TransformaciÃ³n};
    C --> D[Limpieza, CreaciÃ³n de ID, NormalizaciÃ³n de Nulos];
    D -- Datos Limpios --> E[dataset_artistas_CSV_PARA_BD.csv];
    E -- Importar Data --> F[Carga a Supabase: Tabla Dataset_Ranking];

    F -- Credenciales API --> G[Clonar Repo + Configurar .env];
    G --> H[uv venv / uv sync: Configurar Entorno Virtual];
    H --> I[ConexiÃ³n a Supabase (usando .env)];
    I -- Datos SQL --> J[Notebook ej3u2.ipynb: Trabajar DataFrame];
    J -- CÃ³digo Ejecutado --> K[AnÃ¡lisis, ExploraciÃ³n y VisualizaciÃ³n con Plotly];

    K -- Pruebas y Desarrollo --> L[VisualizaciÃ³n Local (uv run jupyter lab)];
    K -- Despliegue Cloud --> M[Live Demo en Deepnote.com];

    %% Aplicar Estilos a las Secciones
    class A, B source;
    class C, D, E, F etl;
    class G, H, I, J dev;
    class K, L, M prod;

    %% SubgrÃ¡ficos (AgrupaciÃ³n)
    subgraph 1. Datos Fuente
        A
    end
    subgraph 2. ETL y Base de Datos (Supabase)
        B
        C
        D
        E
        F
    end
    subgraph 3. Desarrollo y AnÃ¡lisis (Entorno UV)
        G
        H
        I
        J
    end
    subgraph 4. VisualizaciÃ³n y ProducciÃ³n
        K
        L
        M
    end